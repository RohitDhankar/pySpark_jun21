{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "9a17dd69",
   "metadata": {},
   "outputs": [],
   "source": [
    "import findspark\n",
    "findspark.init()\n",
    "\n",
    "import pyspark\n",
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "2b1175a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "15866ff1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pyspark.sql.session.SparkSession'>\n"
     ]
    }
   ],
   "source": [
    "spark_session1 = SparkSession.builder.appName('solution').getOrCreate()\n",
    "print(type(spark_session1)) #<class 'pyspark.sql.session.SparkSession'>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "80b901a9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pyspark.sql.dataframe.DataFrame'>\n",
      "--- --- --- --- --- --- --- --- --- --- \n",
      "['Date', 'Open', 'High', 'Low', 'Close', 'Volume', 'Adj Close']\n",
      "--- --- --- --- --- --- --- --- --- --- \n",
      "root\n",
      " |-- Date: string (nullable = true)\n",
      " |-- Open: double (nullable = true)\n",
      " |-- High: double (nullable = true)\n",
      " |-- Low: double (nullable = true)\n",
      " |-- Close: double (nullable = true)\n",
      " |-- Volume: integer (nullable = true)\n",
      " |-- Adj Close: double (nullable = true)\n",
      "\n",
      "None\n",
      "--- --- --- --- --- --- --- --- --- --- \n",
      "[Row(Date='2012-01-03', Open=59.970001, High=61.060001, Low=59.869999, Close=60.330002, Volume=12668800, Adj Close=52.619234999999996), Row(Date='2012-01-04', Open=60.209998999999996, High=60.349998, Low=59.470001, Close=59.709998999999996, Volume=9593300, Adj Close=52.078475), Row(Date='2012-01-05', Open=59.349998, High=59.619999, Low=58.369999, Close=59.419998, Volume=12768200, Adj Close=51.825539)]\n",
      "--- --- --- --- --- --- --- --- --- --- \n",
      "+-------+----------+------------------+-----------------+-----------------+-----------------+-----------------+-----------------+\n",
      "|summary|      Date|              Open|             High|              Low|            Close|           Volume|        Adj Close|\n",
      "+-------+----------+------------------+-----------------+-----------------+-----------------+-----------------+-----------------+\n",
      "|  count|      1258|              1258|             1258|             1258|             1258|             1258|             1258|\n",
      "|   mean|      null| 72.35785375357709|72.83938807631165| 71.9186009594594|72.38844998012726|8222093.481717011|67.23883848728146|\n",
      "| stddev|      null|  6.76809024470826|6.768186808159218|6.744075756255496|6.756859163732991|  4519780.8431556|6.722609449996857|\n",
      "|    min|2012-01-03|56.389998999999996|        57.060001|        56.299999|        56.419998|          2094900|        50.363689|\n",
      "|    max|2016-12-30|         90.800003|        90.970001|            89.25|        90.470001|         80898100|84.91421600000001|\n",
      "+-------+----------+------------------+-----------------+-----------------+-----------------+-----------------+-----------------+\n",
      "\n",
      "--- --- --- --- --- --- --- --- --- --- \n"
     ]
    }
   ],
   "source": [
    "df_walmart = spark_session1.read.csv('walmart_stock.csv', header=True, \n",
    "                    inferSchema=True)\n",
    "print(type(df_walmart)) # <class 'pyspark.sql.dataframe.DataFrame'>\n",
    "print(\"--- \"*10)\n",
    "print(df_walmart.columns)\n",
    "print(\"--- \"*10)\n",
    "print(df_walmart.printSchema())\n",
    "print(\"--- \"*10)\n",
    "print(df_walmart.head(3))\n",
    "print(\"--- \"*10)\n",
    "df_walmart.describe().show()\n",
    "print(\"--- \"*10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "a19abf97",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------------+\n",
      "|        avg(Open)|\n",
      "+-----------------+\n",
      "|72.35785375357709|\n",
      "+-----------------+\n",
      "\n",
      "+-----------------+\n",
      "|        avg(High)|\n",
      "+-----------------+\n",
      "|72.83938807631165|\n",
      "+-----------------+\n",
      "\n",
      "+----------------+\n",
      "|        avg(Low)|\n",
      "+----------------+\n",
      "|71.9186009594594|\n",
      "+----------------+\n",
      "\n",
      "+-----------------+\n",
      "|       avg(Close)|\n",
      "+-----------------+\n",
      "|72.38844998012726|\n",
      "+-----------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import mean\n",
    "df_walmart.select(mean(\"Open\")).show()\n",
    "df_walmart.select(mean(\"High\")).show()\n",
    "df_walmart.select(mean(\"Low\")).show()\n",
    "df_walmart.select(mean(\"Close\")).show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "a5358976",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'col' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-23-9c971e662d56>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mpyspark\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msql\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfunctions\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0msize\u001b[0m \u001b[0;34m,\u001b[0m \u001b[0msplit\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mdf_walmart\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mselect\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msplit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcol\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"High\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\" \"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshow\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m: name 'col' is not defined"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import size , split\n",
    "df_walmart.select(size(split(col(\"High\"), \" \"))).show(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "e2cca80c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "== Physical Plan ==\n",
      "*(1) Sort [High#677 ASC NULLS FIRST], true, 0\n",
      "+- Exchange rangepartitioning(High#677 ASC NULLS FIRST, 200), ENSURE_REQUIREMENTS, [id=#371]\n",
      "   +- FileScan csv [Date#675,Open#676,High#677,Low#678,Close#679,Volume#680,Adj Close#681] Batched: false, DataFilters: [], Format: CSV, Location: InMemoryFileIndex[file:/home/dhankar/temp/0521/pySpark_june21/GitUp_PySpark_June21/GitUp/pySpark_..., PartitionFilters: [], PushedFilters: [], ReadSchema: struct<Date:string,Open:double,High:double,Low:double,Close:double,Volume:int,Adj Close:double>\n",
      "\n",
      "\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "print(df_walmart.sort(\"High\").explain())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7bf1f037",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f0496502",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3.0\n"
     ]
    }
   ],
   "source": [
    "sc = pyspark.SparkContext(appName=\"Pi\")\n",
    "num_samples = 100\n",
    "\n",
    "def inside(p):     \n",
    "  x, y = random.random(), random.random()\n",
    "  return x*x + y*y < 1\n",
    "\n",
    "count = sc.parallelize(range(0, num_samples)).filter(inside).count()\n",
    "\n",
    "pi = 4 * count / num_samples\n",
    "print(pi)\n",
    "\n",
    "sc.stop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c5b09ce",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec35db18",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b9a57f1d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
